{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a12cdd",
   "metadata": {},
   "source": [
    "# Data Scrubbing and Cleaning for Improved Analysis with DataCleaner  \n",
    "\n",
    "Check my Github for [this project's details](https://github.com/westrany/Data-Scrubbing-and-Cleaning-for-Improved-Analysis-with-DataCleaner/blob/main/README.md). More info on the chosen dataset has been [uploaded to Kaggle](https://www.kaggle.com/datasets/mariafitas/goodreads-small-dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4ba02f",
   "metadata": {},
   "source": [
    "---  \n",
    "## Increasing Data Rate Limit  \n",
    "When working on this project, I've often gotten the \"IOPub data rate exceeded\" message. To resolve this, I have increased the notebook's data rate limit by doing the following:  \n",
    "1. Close Jupyter Notebook.  \n",
    "2. Open your terminal/command prompt.  \n",
    "3. Run the following command:  \n",
    "```\n",
    "jupyter notebook --NotebookApp.iopub_data_rate_limit=1000000000\n",
    "```\n",
    "\n",
    "That should have increased the data rate limit to 1 GB/s, and you can now return to working on your notebook.  \n",
    "\n",
    "â†’ Note that if you are using Anaconda to run this Notebook, run that command on Anaconda's *CMD.exe Prompt*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b10e21f",
   "metadata": {},
   "source": [
    "---\n",
    "## Libraries Imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9ec2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d01126",
   "metadata": {},
   "source": [
    "---\n",
    "## **Data Collection**  \n",
    "\n",
    "### **Load CSV Data into DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5532464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For file in the same directory\n",
    "df = pd.read_csv('unclean_goodreads_library_export.csv') \n",
    "\n",
    "# If the CSV file is in a different directory, specify the path\n",
    "# df = pd.read_csv('/path/to/your_file.csv')\n",
    "\n",
    "# If the CSV file is hosted online, provide the URL\n",
    "# df = pd.read_csv('https://example.com/your_file.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56584b4",
   "metadata": {},
   "source": [
    "### **Display original data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7496ce5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set display option to show all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Set display option to show all columns without truncation\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Set display option to show ISBN13 without scientific notation\n",
    "pd.options.display.float_format = '{:.0f}'.format\n",
    "\n",
    "# Display all rows of the DataFrame\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defb7b41",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Initial Data Assessment\n",
    "\n",
    "### **Display the structure of the dataset (columns, data types, dataframe size)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a291718d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display columns and data types\n",
    "print(\"Columns and Data Types:\")\n",
    "display(df.dtypes)\n",
    "\n",
    "print(\"\\nSize of the DataFrame:\")\n",
    "display(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f25c93",
   "metadata": {},
   "source": [
    "### **Change data types to match expectations**  \n",
    "\n",
    "Date columns \"**Date Read**\" and \"**Date Added**\" should be in the DD/MM/YYYY formats.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f7b9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Data Read and Data Added columns to DD/MM/YYYY\n",
    "df['Date Read'] = pd.to_datetime(df['Date Read'], format='%d/%m/%Y', errors='coerce')\n",
    "df['Date Added'] = pd.to_datetime(df['Date Added'], format='%d/%m/%Y', errors='coerce')\n",
    "\n",
    "\n",
    "# Verify changes and check distribution of values\n",
    "print(\"Data type of 'Date Read' column:\", df['Date Read'].dtype, end='\\n\\n')\n",
    "print(\"Data type of 'Date Added' column:\", df['Date Added'].dtype, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002fbe6f",
   "metadata": {},
   "source": [
    "The column \"**Spoiler**\" refers to the existence of book spoilers in the \"My Review\" data. Currently the column marks TRUE when there is a value, and leaves it blank when there are no spoilers. To better analyse this, I'm changing the TRUE values to 1, and the empty cells to 0, thus converting the column to integer values, and then to boolean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11547bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with 0\n",
    "df['Spoiler'] = df['Spoiler'].fillna(0)\n",
    "\n",
    "# Replace 'TRUE' values with 1\n",
    "df.loc[df['Spoiler'] == 'TRUE', 'Spoiler'] = 1\n",
    "\n",
    "# Convert to boolean\n",
    "df['Spoiler'] = df['Spoiler'].astype(bool)\n",
    "\n",
    "# Convert column to categorical type\n",
    "df['Spoiler'] = df['Spoiler'].astype('category')\n",
    "\n",
    "# Verify changes and check distribution of values\n",
    "print(\"Data type of 'Spoiler' column after conversion:\", df['Spoiler'].dtype)\n",
    "print(\"\\nDistribution of values in 'Spoiler' column:\")\n",
    "print(df['Spoiler'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439d90f3",
   "metadata": {},
   "source": [
    "**ISBN** and **ISBN13** are are unique identifiers and have no inherent order, hence why these should be classified as categorical data and not numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7f5090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column to categorical type\n",
    "df['ISBN'] = df['ISBN'].astype('category')\n",
    "df['ISBN13'] = df['ISBN13'].astype('category')\n",
    "\n",
    "# Verify changes and check distribution of values\n",
    "print(\"Data type of 'ISBN' column after conversion:\", df['ISBN'].dtype)\n",
    "print(\"Data type of 'ISBN13' column after conversion:\", df['ISBN13'].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18913467",
   "metadata": {},
   "source": [
    "### **List which collumns are Categorical and which are Numerical values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d134a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_columns(df):\n",
    "    categorical_cols = []\n",
    "    numerical_cols = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if pd.api.types.is_categorical_dtype(df[col]) or pd.api.types.is_string_dtype(df[col]):\n",
    "            categorical_cols.append(col)\n",
    "        elif pd.api.types.is_numeric_dtype(df[col]):\n",
    "            numerical_cols.append(col)\n",
    "    \n",
    "    return categorical_cols, numerical_cols\n",
    "\n",
    "categorical_cols, numerical_cols = classify_columns(df)\n",
    "\n",
    "print(\"Categorical columns:\", categorical_cols, end='\\n\\n')\n",
    "print(\"Numerical columns:\", numerical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0568a25d",
   "metadata": {},
   "source": [
    "### **Check for missing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb75247c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate missing values\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# Create a DataFrame for missing values\n",
    "missing_values_df = missing_values.to_frame().reset_index()\n",
    "missing_values_df.columns = ['Column', 'Missing Values']\n",
    "\n",
    "# Display the missing values DataFrame\n",
    "display(missing_values_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38be282d",
   "metadata": {},
   "source": [
    "### **Visualise missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918f1811",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df.isnull(), cmap='viridis', cbar=False)\n",
    "plt.title('Missing Values Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda90de4",
   "metadata": {},
   "source": [
    "### **Check for duplicate entries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7144f58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check for duplicate entries\n",
    "duplicate_entries = df.duplicated()\n",
    "\n",
    "# Display duplicate entries\n",
    "print(\"Duplicate entries:\")\n",
    "display(df[duplicate_entries])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eaedaa",
   "metadata": {},
   "source": [
    "### **For categorical values, check for unique values and compare how many unique values are per column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a9fe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_cols:\n",
    "    unique_values = df[col].unique()\n",
    "    num_unique = len(unique_values)\n",
    "    total_values = len(df[col])\n",
    "    print(f\"Column: {col}\")\n",
    "    print(f\"Number of unique values: {num_unique}\")\n",
    "    print(f\"Total number of values: {total_values}\", end='\\n\\n')\n",
    "    \n",
    "    # Uncomment the following to see all unique values\n",
    "    # print(f\"Unique values: {unique_values}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f976fc",
   "metadata": {},
   "source": [
    "### **For numerical values, check for unique values and compare how many unique values are per column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8256b0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numerical_cols:\n",
    "    unique_values = df[col].unique()\n",
    "    num_unique = len(unique_values)\n",
    "    total_values = len(df[col])\n",
    "    print(f\"Column: {col}\")\n",
    "    print(f\"Number of unique values: {num_unique}\")\n",
    "    print(f\"Total number of values: {total_values}\", end='\\n\\n')\n",
    "    \n",
    "    # Uncomment the following to see all unique values\n",
    "    # print(f\"Unique values: {unique_values}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c715e3",
   "metadata": {},
   "source": [
    "### **Check for Outliers and Analyse Statistics**  \n",
    "\n",
    "The following boxplots and statistical analysis refer to columns where checking for outliers might be relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319b7a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check = [\"My Rating\", \"Average Rating\", \"Number of Pages\", \"Year Published\", \"Original Publication Year\"]\n",
    "\n",
    "# Plot box plots for each column\n",
    "for col in columns_to_check:\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    sns.boxplot(x=df[col])\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    desc = df[col].describe()\n",
    "    min_val = desc['min']\n",
    "    q1 = desc['25%']\n",
    "    median = desc['50%']\n",
    "    q3 = desc['75%']\n",
    "    max_val = desc['max']\n",
    "    \n",
    "    # Add annotations to the box plot\n",
    "    plt.text(1.05, 0.5, f\"Min: {min_val:.2f}\\nQ1: {q1:.2f}\\nMedian: {median:.2f}\\nQ3: {q3:.2f}\\nMax: {max_val:.2f}\",\n",
    "             transform=plt.gca().transAxes, fontsize=10, verticalalignment='center', horizontalalignment='left')\n",
    "    \n",
    "    plt.title(f'Boxplot of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8050defa",
   "metadata": {},
   "source": [
    "### Check that the structure of the dataset (columns, data types, dataframe size) has all changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108fb285",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns and Data Types:\")\n",
    "display(df.dtypes)\n",
    "\n",
    "print(\"\\nSize of the DataFrame:\")\n",
    "display(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862a374e",
   "metadata": {},
   "source": [
    "## Data Cleaning  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9a0a8d",
   "metadata": {},
   "source": [
    "### Remove duplicate data entries and confirm removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7129185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate entries\n",
    "df = df[~duplicate_entries]\n",
    "\n",
    "# Confirm removal by checking the shape of the DataFrame\n",
    "print(\"Shape after removing duplicates:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0781b670",
   "metadata": {},
   "source": [
    "### Replace missing values with mode, median, NaN or Not assigned  \n",
    "\n",
    "Fill with NaN:\n",
    "\n",
    "- **Additional Authors**: Missing values here indicate the absence of additional authors, so filling with NaN is appropriate.\n",
    "- **My Review**: If there is no review provided, it's logical to fill the missing values with NaN.\n",
    "- **Private Notes**: Similar to My Review, missing values in this column likely indicate the absence of private notes.\n",
    "  \n",
    "Fill with Median:\n",
    "\n",
    "- **Number of Pages**: Filling missing values with the median number of pages can help retain the central tendency of the data without introducing bias.\n",
    "- **Original Publication Year**: Filling missing values with the median publication year can help retain the central tendency of the data without introducing bias.  \n",
    "\n",
    "Fill with Mode:\n",
    "\n",
    "- **Date Read**: If the date of reading is not provided, filling missing values with the mode (most frequent date read) could be appropriate.\n",
    "- **Publisher**: If the publisher information is missing, filling missing values with the mode (most frequent publisher) could be appropriate.\n",
    "- **Binding**: If the binding type is not provided, filling missing values with the mode (most frequent binding type) could be appropriate.  \n",
    "\n",
    "Fill with Not assigned:\n",
    "\n",
    "- **Bookshelves**: If a book is not assigned to any bookshelf, it could be considered as missing information. Hence filling missing values with a new category for not assigned could be appropriate.\n",
    "- **Bookshelves with positions**: similar to the case above where filling the missing values with a new category for not assigned could be appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec71a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill with NaN\n",
    "nan_columns = ['Additional Authors', 'My Review', 'Private Notes']\n",
    "df[nan_columns] = df[nan_columns].fillna(pd.NA)\n",
    "\n",
    "# Fill with median\n",
    "median_columns = ['Number of Pages', 'Original Publication Year']\n",
    "df[median_columns] = df[median_columns].fillna(df[median_columns].median())\n",
    "\n",
    "# Fill with mode\n",
    "mode_columns = ['Date Read', 'Publisher', 'Binding']\n",
    "for col in mode_columns:\n",
    "    df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "\n",
    "# Fill with Not assigned\n",
    "not_assigned_columns = ['Bookshelves', 'Bookshelves with positions']\n",
    "df[not_assigned_columns] = df[not_assigned_columns].fillna('Not assigned')\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b5c2e2",
   "metadata": {},
   "source": [
    "### Enconde categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5782e3b3",
   "metadata": {},
   "source": [
    "- **Title**: One-Hot Encoding is suitable if the number of unique titles is relatively small and manageable. Each title would be represented by a binary vector, where each element corresponds to whether the title matches a particular category or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696a3387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot enconding\n",
    "title_encoded = pd.get_dummies(df['Title'], prefix='Title')\n",
    "\n",
    "# Concatenate the encoded column with the original DF\n",
    "df_encoded = pd.concat([df, title_encoded], axis=1)\n",
    "\n",
    "# Drop the original 'Title' column if needed\n",
    "# df_encoded.drop('Title', axis=1, inplace=True)\n",
    "\n",
    "# Display the encoded DataFrame\n",
    "print(df_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea48982",
   "metadata": {},
   "source": [
    "- **Author**: Target Encoding encodes each author based on the mean of the target variable (e.g., rating) for books written by that author. This can capture the relationship between author and target variable, making it useful for predictive modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb39c863",
   "metadata": {},
   "source": [
    "- **Author l-f**: Target Encoding, same rationale as Author.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbadc71",
   "metadata": {},
   "source": [
    "- **Additional Authors**: Frequency Encoding since missing values here indicate the absence of additional authors, Frequency Encoding can be used to encode each additional author based on its frequency of occurrence in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6799e665",
   "metadata": {},
   "source": [
    "- **ISBN**: Frequency Encoding is preferred as there are many unique ISBN values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1735a6",
   "metadata": {},
   "source": [
    "- **ISBN13**: Frequency Enconding, same rationale as ISBN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1a6959",
   "metadata": {},
   "source": [
    "- **Publisher**: Target Encoding can capture the relationship between publisher and the target variable. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f2b56d",
   "metadata": {},
   "source": [
    "- **Binding**: One-Hot Encoding since Binding types are nominal categories with no ordinal relationship, making One-Hot Encoding suitable for encoding each type as a binary feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52efcf19",
   "metadata": {},
   "source": [
    "- **Bookshelves**: Frequency Enconding as there many multiple unique categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c731ba",
   "metadata": {},
   "source": [
    "- **Bookshelves with positions**: Frequency Encoding as there are multiple unique categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e666c993",
   "metadata": {},
   "source": [
    "- **Exclusive Shelf**: One-Hot Encoding as it is a categorical variable with multiple categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f622c8",
   "metadata": {},
   "source": [
    "- **My Review**: Frequency Encoding as reviews are not categorized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93840c90",
   "metadata": {},
   "source": [
    "- **Spoiler**: One-Hot Encoding as this is a boolean categorical variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9f0ef5",
   "metadata": {},
   "source": [
    "- **Private Notes**: Frequency Encoding as notes are not categorized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78902c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
